
华三、元核云、XSKY

以17.2.0为基础

client和mds的消息



struct MetaRequest
{
  ceph_mds_request_head head;
  filepath path, path2;
}

class MClientRequest   的祖先class Message中有ceph::buffer::list       payload;
{
  decode_payload
  encode_payload
}

struct ceph_mds_request_head head;

Client::make_request
Client::send_request
   Client::build_client_request
      session->con->send_message2   //传入的是MClientRequest，并且已经encode好了payload
	    AsyncConnection::send_message(Message *m)
		   ProtocolV2::send_message
		      ProtocolV2::prepare_send_message  //根据payload准备header和footer


ProtocolV2::write_event             这部分比较模糊
ProtocolV2::write_message




下盘的数据结构见：cinode.h和cinode.cc
                  CInode::encode_store
				  InodeStoreBase::encode_bare

struct inode_t {        //mdstypes.h
  inodeno_t ino = 0;
  uint32_t   rdev = 0;
  utime_t    ctime;
  utime_t    btime;
  uint32_t   mode = 0;
  uid_t      uid = 0;
  gid_t      gid = 0;
  int32_t    nlink = 0;
  ceph_dir_layout dir_layout = {};
  file_layout_t layout;
  compact_set<int64_t, std::less<int64_t>, Allocator<int64_t>> old_pools;
  ...
}


CDentry.h和CDentry.cc

CDentry::encode_remote
CDentry::decode_remote
CDentry::dump


CDir.h和CDir.cc

CDir::commit
CDir::_omap_commit_ops




rename和link： 采用两阶段事务


Server::_link_remote
  op = MMDSPeerRequest::OP_LINKPREP;
  mds->send_message_mds


Server::_commit_peer_link
Server::_commit_peer_rmdir
Server::_commit_peer_rename
Server::_committed_peer

hardlink:
struct inode_backtrace_t

Jerasure2.0: ceph中默认的EC编码就是这个。



王豪迈 (Ceph)
王豪迈,就职于,现任Ceph一职。

豪迈是 XSKY 的 CTO，XSKY 总部在北京。他从 2013 到现在一直在做 Ceph 的全职工作。豪迈在 Kraken 版本的大部分工作是在 AsyncMessenger 上，并将其扩展到多个后端，包括 DPDK 和 InfiniBand 支持。
Ceph中国社区联合创始人耿航

 1 位技术委员会成员(谢型果)、3 位核心贡献者(谢型果、任焕文、严军)
 
 

Ceph的创始人Sage Weil
圆桌会议（Panel Discussion）
峰会(Summit)




在新的元数据服务器上新增的目录树信息要被记录到日志中来持久化，在权限认证信息从旧的服务器转移到新的机器过程中，采用类似2PC的做法，先在两台服务器上写上日志，然后再提交转移，来防止可能的失败。

把inode的内容分为三个组，安全（owner，mode），文件（size，mtime），不可变的（inode number，ctime，layout），不可变的字段是只读的，安全和文件这两组都拥有自己的锁和状态机。


在CEPH系统的MDS服务中，文件系统的元数据以日志形式存放于segment中，每个sgement存放1024个操作记录。当文件系统的数据使用完毕，CEPH系统则会以一定的速度Trim (消除) 元数据信息，此时会将MDS服务器中的数据回写到CEPH的磁盘OSD中，以降低MDS服务器上运行的mds服务的内存消耗。
ceph health detail
ceph config set mds mds_log_max_segments 1024
ceph config set mds mds_cache_trim_threshold 25600000
ceph config set mds mds_cache_trim_decay_rate 0.01


ceph config dump
WHO         MASK  LEVEL     OPTION                                 VALUE                                                                                      RO
global            advanced  cluster_network                        192.168.209.0/24                                                                           *
global            basic     container_image                        quay.io/ceph/ceph@sha256:0560b16bec6e84345f29fb6693cd2430884e6efff16a95d5bdd0bb06d7661c45  *
mon               advanced  auth_allow_insecure_global_id_reclaim  false
mon               advanced  public_network                         192.168.209.0/24                                                                           *
mgr               advanced  mgr/cephadm/container_init             True                                                                                       *
mgr               advanced  mgr/cephadm/migration_current          5                                                                                          *
mgr               advanced  mgr/dashboard/ALERTMANAGER_API_HOST    http://ceph100:9093                                                                        *
mgr               advanced  mgr/dashboard/GRAFANA_API_SSL_VERIFY   false                                                                                      *
mgr               advanced  mgr/dashboard/GRAFANA_API_URL          https://ceph100:3000                                                                       *
mgr               advanced  mgr/dashboard/PROMETHEUS_API_HOST      http://ceph100:9095                                                                        *
mgr               advanced  mgr/dashboard/ssl_server_port          8443                                                                                       *
mgr               advanced  mgr/orchestrator/orchestrator          cephadm
osd               advanced  osd_memory_target_autotune             true
osd.0             basic     osd_mclock_max_capacity_iops_hdd       9818.879412
osd.1             basic     osd_mclock_max_capacity_iops_hdd       16562.075595
osd.2             basic     osd_mclock_max_capacity_iops_hdd       10540.843957
mds.cephfs        basic     mds_join_fs                            cephfs


cephfs-journal-tool journal <inspect|import|export|reset>
cephfs-journal-tool header <get|set>
cephfs-journal-tool event <get|splice|apply> [filter] <list|json|summary>


cephfs-table-tool
cephfs-data-scan



root@ceph100:~# ceph -s
  cluster:
    id:     63f4b6e8-9c8b-11ed-b51b-291c7c855967
    health: HEALTH_WARN
            1 hosts fail cephadm check
            clock skew detected on mon.ceph102
            1/3 mons down, quorum ceph100,ceph102
            Degraded data redundancy: 28/84 objects degraded (33.333%), 14 pgs degraded, 49 pgs undersized
            49 pgs not deep-scrubbed in time
            49 pgs not scrubbed in time

  services:
    mon: 3 daemons, quorum ceph100,ceph102 (age 52m), out of quorum: ceph101
    mgr: ceph102.hhdpjq(active, since 5h), standbys: ceph100.wzncva
    mds: 1/1 daemons up, 1 standby
    osd: 3 osds: 2 up (since 52m), 2 in (since 5h)

  data:
    volumes: 1/1 healthy
    pools:   3 pools, 49 pgs
    objects: 28 objects, 777 KiB
    usage:   49 MiB used, 20 GiB / 20 GiB avail
    pgs:     28/84 objects degraded (33.333%)
             35 active+undersized
             14 active+undersized+degraded





root@ceph100:~# ceph fs status
cephfs - 0 clients
======
RANK  STATE            MDS              ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active  cephfs.ceph100.sxvbgs  Reqs:    0 /s    13     16     13      0
      POOL         TYPE     USED  AVAIL
cephfs_metadata  metadata   237k  14.2G
  cephfs_data      data    8192   14.2G
     STANDBY MDS
cephfs.ceph102.ttmfej
MDS version: ceph version 17.2.5 (98318ae89f1a893a6ded3a640405cdbb33e08757) quincy (stable)








root@ceph100:~#  cephfs-journal-tool --help
Usage:
  cephfs-journal-tool [options] journal <command>
    <command>:
      inspect
      import <path> [--force]
      export <path>
      reset [--force]
  cephfs-journal-tool [options] header <get|set> <field> <value>
    <field>: [trimmed_pos|expire_pos|write_pos|pool_id]
  cephfs-journal-tool [options] event <effect> <selector> <output> [special options]
    <selector>:
      --range=<start>..<end>
      --path=<substring>
      --inode=<integer>
      --type=<UPDATE|OPEN|SESSION...><
      --frag=<ino>.<frag> [--dname=<dentry string>]
      --client=<session id integer>
    <effect>: [get|recover_dentries|splice]
    <output>: [summary|list|binary|json] [--path <path>]

General options:
  --rank=filesystem:mds-rank|all Journal rank (mandatory)
  --journal=<mdlog|purge_queue>  Journal type (purge_queue means
                                 this journal is used to queue for purge operation,
                                 default is mdlog, and only mdlog support event mode)

Special options
  --alternate-pool <name>     Alternative metadata pool to target
                              when using recover_dentries.
  --conf/-c FILE    read configuration from the given configuration file
  --id ID           set ID portion of my name
  --name/-n TYPE.ID set name
  --cluster NAME    set cluster name (default: ceph)
  --setuser USER    set uid to user or uid (and gid to user's gid)
  --setgroup GROUP  set gid to group or gid
  --version         show version and quit





root@ceph100:~# cephfs-journal-tool --rank=cephfs:all journal export /home/journal.txt
journal is 4258843~7875
wrote 7875 bytes at offset 4258843 to /home/journal.txt
NOTE: this is a _sparse_ file; you can
        $ tar cSzf /home/journal.txt.tgz /home/journal.txt
      to efficiently compress it while preserving sparseness.




FileStore
BlueStore


LogEvent
submit_mdlog_entry   -> pending_events  -> submit_thread



CephFS 的元数据中每个目录会作为一个或者多个 Object 存储，类似 . 的命名方式。同时每个 inode 和 dentry 信息也会被内置其中，该类 Object 会被存储到 Metadata Pool 中。












# 删除所有 metadata 中的数据
$ for i in `rados -p cephfs.a.meta ls`; rados -p cephfs.a.meta rm $i; done

# 查看集群状态
$ ceph -s
  cluster:
    id:     399aa358-dc80-4bed-a957-7e27f8943b67
    health: HEALTH_WARN
            1 filesystem is degraded
            insufficient standby MDS daemons available

  services:
    mon: 3 daemons, quorum a,b,c (age 31h)
    mgr: x(active, since 31h)
    mds: 1/1 daemons up
    osd: 3 osds: 3 up (since 31h), 3 in (since 11d)

  data:
    volumes: 0/1 healthy, 1 recovering
    pools:   4 pools, 81 pgs
    objects: 136 objects, 5.5 MiB
    usage:   3.0 GiB used, 300 GiB / 303 GiB avail
    pgs:     81 active+clean

# 查看 fs 状态
$ ceph fs status
a - 0 clients
=
RANK      STATE      MDS  ACTIVITY   DNS    INOS   DIRS   CAPS
 0    replay(laggy)   a                0      0      0      0
     POOL        TYPE     USED  AVAIL
cephfs.a.meta  metadata  1095k  98.9G
cephfs.a.data    data    14.3M  98.9G



# 首先，将现有的文件系统停止，以防止对数据池的进一步修改。卸载所有客户端。
$ ceph fs fail a

# 接下来，创建一个恢复文件系统，我们将在其中填充由原始数据池支持的新元数据池。
$ ceph osd pool create cephfs_recovery_meta
$ ceph fs new cephfs_recovery cephfs_recovery_meta cephfs.a.data  --recover --allow-dangerous-metadata-overlay

# 恢复过程中我们将关闭MDS，因为我们不希望它与元数据池进一步交互。
$ ceph fs fail cephfs_recovery

# 接下来，我们将重置MDS创建的初始元数据:
$ cephfs-table-tool cephfs_recovery:0 reset session
$ cephfs-table-tool cephfs_recovery:0 reset snap
$ cephfs-table-tool cephfs_recovery:0 reset inode
$ cephfs-journal-tool --rank cephfs_recovery:0 journal reset --force

# 现在从数据池中恢复元数据池:
$ cephfs-data-scan init --force-init --filesystem cephfs_recovery --alternate-pool cephfs_recovery_meta
$ cephfs-data-scan scan_extents --alternate-pool cephfs_recovery_meta --filesystem a cephfs.a.data
$ cephfs-data-scan scan_inodes --alternate-pool cephfs_recovery_meta --filesystem a --force-corrupt cephfs.a.data
$ cephfs-data-scan scan_links --filesystem cephfs_recovery

# (注意，配置也可以是全局设置的，也可以是通过ceph.conf文件设置的。)现在，允许MDS加入恢复文件系统:
$ ceph fs set cephfs_recovery joinable true

# 最后，运行前向清除以修复统计信息。确保您有一个MDS正在运行并发出:
$ ceph fs status # get active MDS
$ ceph tell mds.<id> scrub start / recursive repair




$ cephfs-journal-tool --rank=a:0 event recover_dentries list --alternate-pool cephfs_recovery_meta


$ ceph fs status
a - 0 clients
=
RANK  STATE   MDS     ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active   c   Reqs:    0 /s    13     16     12      0
     POOL        TYPE     USED  AVAIL
cephfs.a.meta  metadata   207k  98.9G
cephfs.a.data    data    14.3M  98.9G
cephfs_recovery - 0 clients
===============
RANK  STATE   MDS     ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active   b   Reqs:    0 /s    10     13     12      0
        POOL            TYPE     USED  AVAIL
cephfs_recovery_meta  metadata  96.0k  98.9G
   cephfs.a.data        data    14.3M  98.9G
STANDBY MDS
     a


# 确保整个过程文件系统处于关闭状态
ceph fs fail a
ceph fs set a joinable false

# 接下来，我们将重置MDS创建的初始元数据
cephfs-table-tool a:0 reset session
cephfs-table-tool a:0 reset snap
cephfs-table-tool a:0 reset inode
cephfs-journal-tool --rank a:0 journal reset --force

# 利用数据池和已经创建好的recovery fs恢复元数据池
cephfs-data-scan init --force-init --filesystem a --alternate-pool cephfs.a.meta
cephfs-data-scan scan_extents --alternate-pool cephfs.a.meta --filesystem cephfs_recovery cephfs.a.data
cephfs-data-scan scan_inodes --alternate-pool cephfs.a.meta --filesystem cephfs_recovery --force-corrupt cephfs.a.data
cephfs-data-scan scan_links --filesystem a

# (注意，配置也可以是全局设置的，也可以是通过ceph.conf文件设置的。)现在，允许MDS加入恢复文件系统:
$ ceph fs set a joinable true

# 最后，运行前向清除以修复统计信息。确保您有一个MDS正在运行并发出:
$ ceph fs status # get active MDS
$ ceph tell mds.<id> scrub start / recursive repair
a - 0 clients
=
RANK  STATE   MDS     ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active   c   Reqs:    0 /s    13     16     12      0
     POOL        TYPE     USED  AVAIL
cephfs.a.meta  metadata   207k  98.9G
cephfs.a.data    data    14.3M  98.9G
cephfs_recovery - 0 clients
===============
RANK  STATE   MDS     ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active   b   Reqs:    0 /s    10     13     12      0
        POOL            TYPE     USED  AVAIL
cephfs_recovery_meta  metadata  96.0k  98.9G
   cephfs.a.data        data    14.3M  98.9G
STANDBY MDS
     a
	 
	 
	 






root@ceph100:/mnt/kernel-cephfs/dir3# rados -p cephfs_metadata ls
601.00000000
10000000004.03c00000
602.00000000
10000000004.03600000
600.00000000
603.00000000
10000000004.03e00000
1.00000000.inode         // 根目录inode 可以通过rados -p cephfs_metadata get 1.00000000.inode /home/root_inode.txt 查询内容
200.00000000             // 200开头的是存放日志的对象，这里面存放了日志的header  (class Header)
200.00000001
606.00000000
607.00000000
200.00000005
mds0_openfiles.0           // open文件记录 ？
608.00000000
200.00000006
500.00000001
10000000004.03000000       //普通目录
10000000004.03a00000
10000000004.00000000
604.00000000
500.00000000
10000000004.03800000
mds_snaptable
605.00000000
10000000004.03400000
mds0_inotable
100.00000000
mds0_sessionmap
200.00000003
200.00000002
609.00000000
400.00000000
10000000004.03200000
200.00000004
100.00000000.inode
1.00000000                 // 根目录


flush journal做的事很简单：1，将logsegment对应的脏元数据落盘；2，修剪日志，将已落盘的日志修剪掉


Journaler::trim


