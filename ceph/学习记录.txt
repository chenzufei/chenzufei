
华三、元核云、XSKY

以17.2.0为基础

client和mds的消息



struct MetaRequest
{
  ceph_mds_request_head head;
  filepath path, path2;
}

class MClientRequest   的祖先class Message中有ceph::buffer::list       payload;
{
  decode_payload
  encode_payload
}

struct ceph_mds_request_head head;

Client::make_request
Client::send_request
   Client::build_client_request
      session->con->send_message2   //传入的是MClientRequest，并且已经encode好了payload
	    AsyncConnection::send_message(Message *m)
		   ProtocolV2::send_message
		      ProtocolV2::prepare_send_message  //根据payload准备header和footer


ProtocolV2::write_event             这部分比较模糊
ProtocolV2::write_message




下盘的数据结构见：cinode.h和cinode.cc
                  CInode::encode_store
				  InodeStoreBase::encode_bare

struct inode_t {        //mdstypes.h
  inodeno_t ino = 0;
  uint32_t   rdev = 0;
  utime_t    ctime;
  utime_t    btime;
  uint32_t   mode = 0;
  uid_t      uid = 0;
  gid_t      gid = 0;
  int32_t    nlink = 0;
  ceph_dir_layout dir_layout = {};
  file_layout_t layout;
  compact_set<int64_t, std::less<int64_t>, Allocator<int64_t>> old_pools;
  ...
}


CDentry.h和CDentry.cc

CDentry::encode_remote
CDentry::decode_remote
CDentry::dump


CDir.h和CDir.cc

CDir::commit
CDir::_omap_commit_ops




rename和link： 采用两阶段事务


Server::_link_remote
  op = MMDSPeerRequest::OP_LINKPREP;
  mds->send_message_mds


Server::_commit_peer_link
Server::_commit_peer_rmdir
Server::_commit_peer_rename
Server::_committed_peer

hardlink:
struct inode_backtrace_t

Jerasure2.0: ceph中默认的EC编码就是这个。



王豪迈 (Ceph)
王豪迈,就职于,现任Ceph一职。

豪迈是 XSKY 的 CTO，XSKY 总部在北京。他从 2013 到现在一直在做 Ceph 的全职工作。豪迈在 Kraken 版本的大部分工作是在 AsyncMessenger 上，并将其扩展到多个后端，包括 DPDK 和 InfiniBand 支持。
Ceph中国社区联合创始人耿航

 1 位技术委员会成员(谢型果)、3 位核心贡献者(谢型果、任焕文、严军)
 
 

Ceph的创始人Sage Weil
圆桌会议（Panel Discussion）
峰会(Summit)




在新的元数据服务器上新增的目录树信息要被记录到日志中来持久化，在权限认证信息从旧的服务器转移到新的机器过程中，采用类似2PC的做法，先在两台服务器上写上日志，然后再提交转移，来防止可能的失败。

把inode的内容分为三个组，安全（owner，mode），文件（size，mtime），不可变的（inode number，ctime，layout），不可变的字段是只读的，安全和文件这两组都拥有自己的锁和状态机。


在CEPH系统的MDS服务中，文件系统的元数据以日志形式存放于segment中，每个sgement存放1024个操作记录。当文件系统的数据使用完毕，CEPH系统则会以一定的速度Trim (消除) 元数据信息，此时会将MDS服务器中的数据回写到CEPH的磁盘OSD中，以降低MDS服务器上运行的mds服务的内存消耗。
ceph health detail
ceph config set mds mds_log_max_segments 1024
ceph config set mds mds_cache_trim_threshold 25600000
ceph config set mds mds_cache_trim_decay_rate 0.01


ceph config dump
WHO         MASK  LEVEL     OPTION                                 VALUE                                                                                      RO
global            advanced  cluster_network                        192.168.209.0/24                                                                           *
global            basic     container_image                        quay.io/ceph/ceph@sha256:0560b16bec6e84345f29fb6693cd2430884e6efff16a95d5bdd0bb06d7661c45  *
mon               advanced  auth_allow_insecure_global_id_reclaim  false
mon               advanced  public_network                         192.168.209.0/24                                                                           *
mgr               advanced  mgr/cephadm/container_init             True                                                                                       *
mgr               advanced  mgr/cephadm/migration_current          5                                                                                          *
mgr               advanced  mgr/dashboard/ALERTMANAGER_API_HOST    http://ceph100:9093                                                                        *
mgr               advanced  mgr/dashboard/GRAFANA_API_SSL_VERIFY   false                                                                                      *
mgr               advanced  mgr/dashboard/GRAFANA_API_URL          https://ceph100:3000                                                                       *
mgr               advanced  mgr/dashboard/PROMETHEUS_API_HOST      http://ceph100:9095                                                                        *
mgr               advanced  mgr/dashboard/ssl_server_port          8443                                                                                       *
mgr               advanced  mgr/orchestrator/orchestrator          cephadm
osd               advanced  osd_memory_target_autotune             true
osd.0             basic     osd_mclock_max_capacity_iops_hdd       9818.879412
osd.1             basic     osd_mclock_max_capacity_iops_hdd       16562.075595
osd.2             basic     osd_mclock_max_capacity_iops_hdd       10540.843957
mds.cephfs        basic     mds_join_fs                            cephfs


cephfs-journal-tool journal <inspect|import|export|reset>
cephfs-journal-tool header <get|set>
cephfs-journal-tool event <get|splice|apply> [filter] <list|json|summary>